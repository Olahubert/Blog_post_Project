{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1958dfcf",
   "metadata": {},
   "source": [
    "# Safeguarding AI Weights: Understanding and Protecting Against Attacks\n",
    "\n",
    "Artificial Intelligence (AI) models are increasingly becoming integral parts of various systems, from simple recommendation engines to complex autonomous vehicles. The core of these models lies in their weights — parameters that the model learns from training data to make predictions or decisions. Understanding AI weights and knowing how to protect them from adversarial attacks is crucial for maintaining the integrity and security of AI systems.\n",
    "\n",
    "What are AI Weights?\n",
    "\n",
    "AI weights are the adjustable parameters within a neural network that are used to minimize the error in the model’s predictions. These weights are fine-tuned during the training process through a mechanism called backpropagation. They essentially determine how input data is transformed as it passes through the network layers to produce the final output.\n",
    "\n",
    "In simpler terms, think of AI weights as the memory of the model. They store the knowledge gained from the training data, which the model uses to make decisions. The arrangement and values of these weights can be seen as the “intelligence” of the AI.\n",
    "\n",
    "\n",
    "AI Weights\n",
    "The Role of Weights in AI Models\n",
    "\n",
    "Learning Patterns: Weights help the model learn from the training data by adjusting based on the error margins.\n",
    "Decision Making: They determine how inputs are transformed and combined at each layer, influencing the final output.\n",
    "Model Adaptation: The fine-tuning of weights allows the model to adapt and generalize from the training data to unseen data.\n",
    "Risks of Unprotected AI Weights\n",
    "\n",
    "Performance Degradation: Altered weights can lead to incorrect predictions or classifications, affecting the reliability of the AI system.\n",
    "Privacy Violations: Adversaries can infer sensitive information from the weights, potentially breaching data privacy.\n",
    "Security Breaches: Manipulated weights can introduce backdoors, allowing adversaries to exploit the AI system at will.\n",
    "Ethical Concerns: Biases can be introduced, leading to unfair or discriminatory outcomes.\n",
    "Types of Adversarial Attacks on AI Weights\n",
    "\n",
    "Evasion Attacks\n",
    "\n",
    "Evasion attacks involve crafting input data that is designed to mislead the model into making incorrect predictions. This type of attack doesn’t involve altering the weights directly but exploits the model’s learned decision boundaries. For instance, in image classification, an attacker might slightly modify an image in a way that is imperceptible to humans but causes the model to misclassify it.\n",
    "\n",
    "Poisoning Attacks\n",
    "\n",
    "In poisoning attacks, the adversary manipulates the training data to corrupt the learned weights. This can lead to a model that performs well on training data but poorly on real-world data. Poisoning attacks are particularly insidious because they can be difficult to detect and can significantly degrade the model’s performance.\n",
    "\n",
    "Model Inversion Attacks\n",
    "\n",
    "Model inversion attacks attempt to reconstruct the training data from the model’s weights, leading to privacy violations. By querying the model and analyzing the responses, an attacker can infer sensitive information about the training data.\n",
    "\n",
    "Weight Manipulation Attacks\n",
    "\n",
    "Weight manipulation attacks involve directly altering the model’s weights to degrade its performance or insert backdoors that can be exploited later. These attacks can be particularly damaging as they can be used to covertly insert vulnerabilities into the model. An example is an autonomous vehicle. An attacker could manipulate the weights of the AI system controlling the vehicle, leading to erratic or dangerous driving behaviors.\n",
    "\n",
    "Strategies to Protect AI Weights\n",
    "\n",
    "1. Regularization Techniques\n",
    "\n",
    "Regularization methods such as L2 regularization can help reduce the model’s complexity and make it less sensitive to small changes in input data. This can indirectly protect against some forms of evasion attacks by making the decision boundaries smoother and less susceptible to adversarial perturbations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10a4e137",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\visit\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,344</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m1,344\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m4,160\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,569</span> (21.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,569\u001b[0m (21.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,569</span> (21.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,569\u001b[0m (21.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(64, input_dim=20, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "    Dense(64, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Summary of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab379508",
   "metadata": {},
   "source": [
    "# 2. Adversarial Training\n",
    "\n",
    "Adversarial training involves augmenting the training dataset with adversarial examples. This helps the model learn to recognize and resist adversarial inputs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c3e7c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\visit\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 10ms/step - accuracy: 0.8913 - loss: 0.3536\n",
      "Epoch 2/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 9ms/step - accuracy: 0.9851 - loss: 0.0499\n",
      "Epoch 3/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 9ms/step - accuracy: 0.9906 - loss: 0.0295\n",
      "Epoch 4/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 10ms/step - accuracy: 0.9939 - loss: 0.0195\n",
      "Epoch 5/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 9ms/step - accuracy: 0.9951 - loss: 0.0155\n",
      "Epoch 1/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 7ms/step - accuracy: 0.8406 - loss: 0.5009\n",
      "Epoch 2/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 9ms/step - accuracy: 0.9645 - loss: 0.1101\n",
      "Epoch 3/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 9ms/step - accuracy: 0.9782 - loss: 0.0641\n",
      "Epoch 4/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 9ms/step - accuracy: 0.9858 - loss: 0.0427\n",
      "Epoch 5/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.9908 - loss: 0.0283\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.5274 - loss: 2.5055\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.634542465209961, 0.5260999798774719]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.reshape(-1, 28, 28, 1).astype('float32') / 255\n",
    "x_test = x_test.reshape(-1, 28, 28, 1).astype('float32') / 255\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "def create_model():\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5, batch_size=32)\n",
    "\n",
    "def create_adversarial_pattern(model, input_image, input_label):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(input_image)\n",
    "        prediction = model(input_image)\n",
    "        loss = tf.keras.losses.categorical_crossentropy(input_label, prediction)\n",
    "        gradient = tape.gradient(loss, input_image)\n",
    "        signed_grad = tf.sign(gradient)\n",
    "    return signed_grad\n",
    "\n",
    "x_train_tensor = tf.convert_to_tensor(x_train)\n",
    "y_train_tensor = tf.convert_to_tensor(y_train)\n",
    "\n",
    "adversarial_examples = create_adversarial_pattern(model, x_train_tensor, y_train_tensor)\n",
    "x_train_adv = x_train_tensor + adversarial_examples\n",
    "\n",
    "x_train_adv = tf.clip_by_value(x_train_adv, 0, 1)\n",
    "\n",
    "model.fit(x_train_adv, y_train_tensor, epochs=5, batch_size=32)\n",
    "\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9d30e6",
   "metadata": {},
   "source": [
    "# 3. Differential Privacy\n",
    "Differential privacy adds noise to the training data or gradients, which makes it difficult for an adversary to extract individual data points from the trained model."
   ]
  },
  {
   "cell_type": "raw",
   "id": "92d98063",
   "metadata": {},
   "source": [
    "import tensorflow_privacy\n",
    "from tensorflow_privacy.privacy.optimizers.dp_optimizer_keras import DPKerasSGDOptimizer\n",
    "optimizer = DPKerasSGDOptimizer(\n",
    "l2_norm_clip=1.0,\n",
    "noise_multiplier=1.1,\n",
    "num_microbatches=256,\n",
    "learning_rate=0.15\n",
    ")\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=5, batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f65c50",
   "metadata": {},
   "source": [
    "# 4. Secure Model Deployment\n",
    "Deploying models in a secure environment using techniques such as secure multi-party computation (SMPC) or homomorphic encryption can protect the model weights during inference. SMPC allows multiple parties to jointly compute a function over their inputs while keeping those inputs private. Homomorphic encryption enables computations on encrypted data, so the data and the results remain encrypted throughout the computation process.\n",
    "5. Model Watermarking\n",
    "Watermarking AI models involves embedding a secret signature into the model's weights that can be used to prove ownership or detect tampering. This is akin to digital watermarks used in images and videos. For AI models, watermarking can be implemented by training the model on a special dataset that includes the watermark. This dataset does not affect the model's primary task but can be used to verify the model's integrity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34708da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def embed_watermark(weights, watermark):\n",
    "    np.random.seed(42)\n",
    "    noise = np.random.normal(0, 0.01, weights.shape)\n",
    "    weights += noise * watermark\n",
    "    return weights\n",
    "\n",
    "model_weights = model.get_weights()\n",
    "watermark = np.random.uniform(-1, 1, model_weights[0].shape)\n",
    "model_weights[0] = embed_watermark(model_weights[0], watermark)\n",
    "model.set_weights(model_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c14e8a5",
   "metadata": {},
   "source": [
    "# 6. Robustness Testing\n",
    "Regularly testing the model for robustness against adversarial examples and other forms of attacks can help in identifying and mitigating vulnerabilities early. This can be done using tools like the Adversarial Robustness Toolbox (ART), which provides functionalities for generating adversarial examples, training models with adversarial training, and evaluating model robustness.\n",
    "7. Model Distillation\n",
    "Model distillation is a technique where a simpler model (student) is trained to mimic the behavior of a more complex model (teacher). This can sometimes make the student model more robust to adversarial attacks. The distillation process involves transferring knowledge from the teacher model to the student model, usually by training the student model on a soft-target output provided by the teacher model."
   ]
  },
  {
   "cell_type": "raw",
   "id": "0f4e92ab",
   "metadata": {},
   "source": [
    "import tensorflow as tf\n",
    "teacher_model = tf.keras.models.load_model('teacher_model.h5')\n",
    "student_model = Sequential([\n",
    "Dense(32, input_dim=20, activation='relu'),\n",
    "Dense(32, activation='relu'),\n",
    "Dense(1, activation='sigmoid')\n",
    "])\n",
    "student_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "def distillation_loss(y_true, y_pred, temperature=10):\n",
    "y_true = tf.nn.softmax(y_true / temperature)\n",
    "y_pred = tf.nn.softmax(y_pred / temperature)\n",
    "return tf.reduce_mean(tf.keras.losses.categorical_crossentropy(y_true, y_pred))\n",
    "student_model.fit(x_train, y_train, epochs=5, batch_size=32, loss=distillation_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9b2a3b",
   "metadata": {},
   "source": [
    "# 8. Continuous Monitoring and Updating\n",
    "Implementing continuous monitoring and updating mechanisms can help in detecting and responding to adversarial attacks in real-time. By continuously evaluating the model's performance and updating it with new data, organizations can ensure that their AI systems remain resilient against evolving threats.\n",
    "9. Federated Learning\n",
    "Federated learning is a decentralized approach where multiple clients collaboratively train a model while keeping their data localized. This technique can enhance the privacy and security of the training data, as the data never leaves the client's device. Federated learning can help in protecting AI weights by reducing the risk of data leakage and making it more challenging for adversaries to perform poisoning attacks.\n",
    "10. Hardware-Based Security Measures\n",
    "Implementing hardware-based security measures such as Trusted Execution Environments (TEEs) can provide an additional layer of protection for AI weights. TEEs are secure areas within a processor that ensure the confidentiality and integrity of the data and code being executed. By running the AI model within a TEE, organizations can protect the model's weights from unauthorized access and tampering.\n",
    "Detailed Example of Protecting AI Weights with Adversarial Training\n",
    "To give a more detailed example, let's walk through a comprehensive Python code implementation of adversarial training using the TensorFlow library. This example uses a simple Convolutional Neural Network (CNN) for image classification on the MNIST dataset."
   ]
  },
  {
   "cell_type": "raw",
   "id": "48b406dd",
   "metadata": {},
   "source": [
    "#Step 1:Import Necessary Libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "#Step 2: Load and Preprocess the Data\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.reshape(-1, 28, 28, 1).astype('float32') / 255\n",
    "x_test = x_test.reshape(-1, 28, 28, 1).astype('float32') / 255\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "#Step 3: Define the CNN Model\n",
    "def create_model():\n",
    "model = tf.keras.Sequential([\n",
    "tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "tf.keras.layers.Flatten(),\n",
    "tf.keras.layers.Dense(64, activation='relu'),\n",
    "tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "return model\n",
    "model = create_model()\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#Step 4: Define the Adversarial Pattern Creation Function\n",
    "def create_adversarial_pattern(model, input_image, input_label):\n",
    "with tf.GradientTape() as tape:\n",
    "tape.watch(input_image)\n",
    "prediction = model(input_image)\n",
    "loss = tf.keras.losses.categorical_crossentropy(input_label, prediction)\n",
    "gradient = tape.gradient(loss, input_image)\n",
    "signed_grad = tf.sign(gradient)\n",
    "return signed_grad\n",
    "#Step 5: Generate Adversarial Examples\n",
    "def generate_adversarial_examples(model, x_data, y_data):\n",
    "adversarial_examples = []\n",
    "for i in range(len(x_data)):\n",
    "perturbation = create_adversarial_pattern(model, x_data[i:i+1], y_data[i:i+1])\n",
    "adversarial_example = x_data[i] + 0.1 * perturbation\n",
    "adversarial_examples.append(adversarial_example.numpy())\n",
    "return np.array(adversarial_examples)\n",
    "x_train_adv = generate_adversarial_examples(model, x_train, y_train)\n",
    "x_test_adv = generate_adversarial_examples(model, x_test, y_test)\n",
    "#Step 6: Combine Original and Adversarial Data\n",
    "x_train_combined = np.concatenate((x_train, x_train_adv))\n",
    "y_train_combined = np.concatenate((y_train, y_train))\n",
    "x_test_combined = np.concatenate((x_test, x_test_adv))\n",
    "y_test_combined = np.concatenate((y_test, y_test))\n",
    "#Step 7: Train the Model with Combined Data\n",
    "model.fit(x_train_combined, y_train_combined, epochs=10, batch_size=64, validation_data=(x_test_combined, y_test_combined))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72b95c2",
   "metadata": {},
   "source": [
    "# By following these steps, you can effectively use adversarial training to enhance the robustness of your AI model against adversarial attacks.\n",
    "Conclusion\n",
    "The security of AI weights is a crucial aspect of ensuring the reliability and trustworthiness of AI systems. As adversarial attacks become more sophisticated, it is essential to adopt a multi-faceted approach to protect AI weights. Regularization, adversarial training, differential privacy, secure deployment, model watermarking, robustness testing, model distillation, continuous monitoring and updating, federated learning, and hardware-based security measures are some of the strategies that can be employed.\n",
    "By implementing these strategies, organizations can enhance the resilience of their AI systems against adversarial threats. Protecting AI weights not only ensures the performance and reliability of AI models but also helps in maintaining the trust of users and stakeholders in AI-driven systems.\n",
    "This post provides a comprehensive overview of AI weights and the various strategies to protect them from adversarial attacks. By understanding and implementing these protection mechanisms, you can significantly enhance the security of your AI models.\n",
    "About me\n",
    "I am a Ph.D. candidate specializing in Generative AI, Machine Learning, AI Assurance, and Responsible AI, with a focus on Adversarial AI. Additionally, I serve as an Adjunct Instructor. My professional background encompasses extensive experience in IT risk and compliance, Governance, Risk Management, and Compliance (GRC), as well as Third Party Risk Management (TPRM).\n",
    "www.linkedin.com/in/olawale-omoyeni-148b851b2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
